{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cJA6wYaoui8"
      },
      "source": [
        "# Single Frame Process for Training a BLIP model for DriveLM Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phNz84jho5Te"
      },
      "source": [
        "## Installing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2sB6CxxoiM1",
        "outputId": "08043d49-9379-4dcf-adb7-566b01aad366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.27.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install peft\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ovoDcEo26t",
        "outputId": "7b4feda0-8ea9-4b4c-b1bd-734b0670e9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6939emoaNm7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering, \\\n",
        "TrainingArguments, Trainer, BertTokenizerFast, Blip2ForConditionalGeneration, Blip2Processor, AutoTokenizer\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from peft import LoraConfig, get_peft_model, LoftQConfig\n",
        "from copy import deepcopy\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import asyncio\n",
        "import pdb\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-yKXaNOIkoC",
        "outputId": "6d93b3ec-65c8-4758-d498-1bb41624b075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘DriveLM’: File exists\n",
            "mkdir: cannot create directory ‘DriveLM/results’: File exists\n",
            "replace DriveLM/data/multi_frame/multi_frame_test.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!mkdir DriveLM\n",
        "!mkdir DriveLM/results\n",
        "!unzip -q drive/MyDrive/DriveLM/data.zip -d DriveLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLHgc3o41Uvr"
      },
      "outputs": [],
      "source": [
        "class SingleFrameDataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_file, processor, custom_train=True):\n",
        "        with open(input_file) as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        # Make processors for Image-Question pairs and Answer text\n",
        "        self.processor = processor\n",
        "\n",
        "        self.custom_train = custom_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the question and answer at the idx\n",
        "        qa, img_path = self.data[idx]\n",
        "        img_path = os.path.join('DriveLM', '/'.join(img_path.split('\\\\')))\n",
        "        q_text, a_text = qa['Q'], qa['A']\n",
        "        new_q_text = f\"Question: {q_text} Answer:\"\n",
        "        full_text = f\"Question: {q_text} Answer: {a_text}\"\n",
        "\n",
        "        return full_text, img_path, a_text, new_q_text\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        full_texts, img_paths, a_texts, q_texts = zip(*batch)\n",
        "        N = len(img_paths)\n",
        "        imgs = [read_image(img_path) for img_path in img_paths]\n",
        "\n",
        "        # Perform this so Q & A are properly padded\n",
        "        # q_texts, a_texts = list(q_texts), list(a_texts)\n",
        "        # q_texts.extend(a_texts)\n",
        "\n",
        "        encodings = self.processor(images=imgs, text=full_texts, padding=True, return_tensors=\"pt\")\n",
        "        q_enc = self.processor(text=q_texts, padding=True, return_tensors='pt').to(device)\n",
        "\n",
        "        # labels = encodings['input_ids'][N:]\n",
        "        # encodings['input_ids'] = encodings['input_ids'][:N]\n",
        "        # encodings['attention_mask'] = encodings['attention_mask'][:N]\n",
        "\n",
        "        # labels = self.processor(text=a_texts, padding=True, return_tensors='pt')\n",
        "\n",
        "\n",
        "        if self.custom_train:\n",
        "            encodings = encodings.to(device)\n",
        "            q_enc = q_enc.to(device)\n",
        "\n",
        "        encodings = {k: v for k, v in encodings.items()}\n",
        "        q_enc = {k: v for k, v in q_enc.items()}\n",
        "        encodings['labels'] = deepcopy(encodings['input_ids'])\n",
        "        # encodings['labels'] = labels\n",
        "\n",
        "        return encodings, q_enc\n",
        "        # return encodings\n",
        "\n",
        "    def test_collate_fn(self, batch):\n",
        "\n",
        "        q_texts, img_paths, a_texts = zip(*batch)\n",
        "        imgs = [read_image(img_path) for img_path in img_paths]\n",
        "\n",
        "        encodings = self.processor(imgs, q_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        labels = self.tokenizer(a_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "        if self.custom_train:\n",
        "            encodings = encodings.to(device)\n",
        "            # labels = labels.to(device)\n",
        "\n",
        "        encodings = {k: v.squeeze() for k, v in encodings.items()}\n",
        "        labels = {k: v.squeeze() for k, v in labels.items()}\n",
        "        encodings['labels'] = labels['input_ids']\n",
        "\n",
        "        return encodings, img_paths, q_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m5KtECVb6fN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "EPOCHS = 1\n",
        "LORA_DIM, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.05\n",
        "NUM_WORKERS = 0\n",
        "LOAD_CHECKPOINT = False\n",
        "CHECKPOINT_FILE = '20240215-044630'\n",
        "CUSTOM_TRAIN = True\n",
        "LORA_ENABLED = True\n",
        "MODEL_TYPE = 'BLIP2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpOzU7D65y4x"
      },
      "outputs": [],
      "source": [
        "def save_model(model, model_name):\n",
        "    # Save the model into the designated folder\n",
        "    path = os.path.join('drive', 'MyDrive', 'DriveLM', 'results', timestr, model_name + '.pth')\n",
        "    torch.save(model, path)\n",
        "\n",
        "\n",
        "def val_model(dloader):\n",
        "\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      model.eval()\n",
        "      for idx, (batch, _) in tqdm(enumerate(dloader), total=len(dloader)):\n",
        "          outputs = model(**batch)\n",
        "          val_loss += outputs.loss.item()\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def save_stats(train_loss, val_loss, epochs):\n",
        "    stats_dict = {\n",
        "        'losses': losses,\n",
        "        'val losses': val_losses,\n",
        "        'min train loss': train_loss,\n",
        "        'min val loss': val_loss,\n",
        "        'epochs': epochs\n",
        "    }\n",
        "\n",
        "    # Save stats into checkpoint\n",
        "    with open(os.path.join('drive', 'MyDrive', 'DriveLM', 'results', timestr, 'stats.json'), 'w') as f:\n",
        "        json.dump(stats_dict, f)\n",
        "\n",
        "\n",
        "def plot_loss(training_loss):\n",
        "    num_epochs = len(training_loss)\n",
        "\n",
        "    plt.plot(range(1, num_epochs + 1), training_loss, label='Training Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Num epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join('drive', 'MyDrive', 'DriveLM', 'results', timestr, 'loss.png'))\n",
        "\n",
        "\n",
        "def custom_train(train_loss, val_loss, best_model, epochs):\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs, EPOCHS):\n",
        "        print('-------------------- EPOCH ' + str(epoch) + ' ---------------------')\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for step, (batch, q_texts) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "\n",
        "              # Forward pass through model\n",
        "              outputs = model(**batch)\n",
        "\n",
        "              # Calculate loss\n",
        "              loss = outputs.loss\n",
        "\n",
        "              # print(loss.item())\n",
        "              # print(outputs.logits)\n",
        "              # print(loss, batch['input_ids'].shape, batch['labels'].shape, batch.keys())\n",
        "              epoch_loss += loss.item()\n",
        "\n",
        "              if step % 500 == 0:\n",
        "                print()\n",
        "                print('Loss: ' + str(loss.item()))\n",
        "\n",
        "                with torch.no_grad():\n",
        "                  outputs = model.generate(**q_texts, pixel_values=batch['pixel_values'], max_length=100)\n",
        "                  # outputs = model.generate(**batch, max_length=400)\n",
        "                  text_outputs = [processor.decode(output.to('cpu'), skip_special_tokens=True) for output in outputs]\n",
        "                  text_questions = [processor.decode(q.to('cpu'), skip_special_tokens=True) for q in q_texts['input_ids']]\n",
        "                  text_labels = [processor.decode(a.to('cpu'), skip_special_tokens=True) for a in batch['labels']]\n",
        "                print()\n",
        "                print('Questions:')\n",
        "                print(text_questions)\n",
        "                print()\n",
        "                print('Generated Answers:')\n",
        "                print(text_outputs)\n",
        "                print()\n",
        "                print('Ground Truth Answers:')\n",
        "                print(text_labels)\n",
        "\n",
        "              # Back-prop\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "        save_model(model.state_dict(), 'latest_model')\n",
        "\n",
        "        # Get train and val loss per batch\n",
        "        epoch_train_loss = epoch_loss / len(train_dataloader)\n",
        "\n",
        "        epoch_val_loss = val_model(val_dataloader) / len(val_dataloader)\n",
        "\n",
        "        if not val_loss or min(epoch_val_loss, val_loss) == epoch_val_loss:\n",
        "            val_loss = epoch_val_loss\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "        if not train_loss or min(train_loss, epoch_train_loss) == epoch_train_loss:\n",
        "            train_loss = epoch_train_loss\n",
        "\n",
        "        # Add losses to epoch list\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        losses.append(epoch_train_loss)\n",
        "\n",
        "        # Adjust learning rate scheduler\n",
        "        # scheduler.step()\n",
        "\n",
        "        print('Training Loss: ' + str(epoch_train_loss))\n",
        "        print('Validation Loss: ' + str(epoch_val_loss))\n",
        "        print('---------------------------------------------')\n",
        "\n",
        "        # Save model and stats for checkpoints\n",
        "        save_model(best_model, 'latest_model')\n",
        "        epochs += 1\n",
        "        save_stats(train_loss, val_loss, epochs)\n",
        "\n",
        "    # Save the model and plot the loss\n",
        "    save_model(best_model, 'latest_model')\n",
        "    plot_loss(losses)\n",
        "\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def train():\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"agopalkr/EfficientDriveLM\",\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dset,\n",
        "        eval_dataset=val_dset,\n",
        "        data_collator=train_dset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    model.push_to_hub(\"agopalkr/EfficientDriveLM\")\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def save_experiment(statistics):\n",
        "    \"\"\"\n",
        "    Saves the experiment results to a csv\n",
        "    :param args: The hyperparameters used\n",
        "    :param statistics: The accuracies for the training, validation, and test sets\n",
        "    \"\"\"\n",
        "    trial_dict = {\n",
        "        'Model name': [timestr],\n",
        "        'Model type': [MODEL_TYPE],\n",
        "        'Learning rate': [LEARNING_RATE],\n",
        "        'Weight decay': [WEIGHT_DECAY],\n",
        "        'Batch size': [BATCH_SIZE],\n",
        "        'Epochs': [EPOCHS],\n",
        "        'LoRA Dimension': [LORA_DIM],\n",
        "        'LoRA Alpha': [LORA_ALPHA],\n",
        "        'LoRA Dropout': [LORA_DROPOUT],\n",
        "        'Min Training Loss': [statistics[0]],\n",
        "        'Min Validation Loss': [statistics[1]],\n",
        "        'Min Testing Loss': [statistics[2]],\n",
        "    }\n",
        "\n",
        "    trial_dict = pd.DataFrame(trial_dict)\n",
        "    trial_dict.to_csv(os.path.join('drive', 'MyDrive', 'DriveLM', 'results', timestr, 'results.csv'), index=False, header=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    min_train_loss = None\n",
        "    min_val_loss = None\n",
        "    best_model = None\n",
        "    epochs_ran = 0\n",
        "\n",
        "    if MODEL_TYPE == 'BLIP':\n",
        "      # Load processors and models\n",
        "      processor = BlipProcessor.from_pretrained('Salesforce/blip-vqa-base')\n",
        "      model = BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base')\n",
        "    else:\n",
        "      processor = Blip2Processor.from_pretrained('Salesforce/blip2-opt-2.7b')\n",
        "      processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
        "      model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b', device_map=\"auto\")\n",
        "\n",
        "    train_dset = SingleFrameDataset(\n",
        "        input_file=os.path.join('drive', 'MyDrive', 'DriveLM', 'sf_train.json'),\n",
        "        processor=processor,\n",
        "        custom_train=CUSTOM_TRAIN\n",
        "    )\n",
        "    val_dset = SingleFrameDataset(\n",
        "        input_file=os.path.join('drive', 'MyDrive', 'DriveLM',\n",
        "                                'sf_val.json'),\n",
        "        processor=processor,\n",
        "        custom_train=CUSTOM_TRAIN\n",
        "    )\n",
        "    test_dset = SingleFrameDataset(\n",
        "        input_file=os.path.join('drive', 'MyDrive', 'DriveLM',\n",
        "                                'sf_test.json'),\n",
        "        processor=processor,\n",
        "        custom_train=CUSTOM_TRAIN\n",
        "    )\n",
        "\n",
        "    print(len(train_dset), len(val_dset), len(test_dset))\n",
        "\n",
        "    # Create Dataloaders\n",
        "    train_dataloader = DataLoader(train_dset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
        "                                  collate_fn=train_dset.collate_fn)\n",
        "    val_dataloader = DataLoader(val_dset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
        "                                collate_fn=val_dset.collate_fn, drop_last=True)\n",
        "    test_dataloader = DataLoader(test_dset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
        "                                 collate_fn=test_dset.collate_fn, drop_last=True)\n",
        "\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Create LoRA model\n",
        "    if LORA_ENABLED:\n",
        "      # For quantization\n",
        "      loftq_config = LoftQConfig(loftq_bits=8)\n",
        "      lora_config = LoraConfig(loftq_config=loftq_config)\n",
        "      model = get_peft_model(model, lora_config)\n",
        "      print_trainable_parameters(model)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    if CUSTOM_TRAIN:\n",
        "\n",
        "        # Load checkpoint if neccesary:\n",
        "        if LOAD_CHECKPOINT:\n",
        "\n",
        "            print('Loading model from ' + CHECKPOINT_FILE)\n",
        "\n",
        "            # Load the model and stats from the checkpoint\n",
        "            model.load_state_dict(torch.load(os.path.join('drive', 'MyDrive', 'DriveLM', 'results', CHECKPOINT_FILE, 'latest_model.pth')))\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "\n",
        "            with open(os.path.join('drive', 'MyDrive', 'DriveLM', 'results', CHECKPOINT_FILE, 'stats.json'), 'r') as f:\n",
        "                stats = json.load(f)\n",
        "\n",
        "            min_train_loss, min_val_loss, losses, val_losses, epochs_ran = stats['min train loss'], \\\n",
        "                                                                           stats['min val loss'], \\\n",
        "                                                                           stats['losses'], stats['val losses'], \\\n",
        "                                                                           stats['epochs']\n",
        "            print(f'Minimum Training Loss: {min_train_loss}')\n",
        "            print(f'Training Losses: {losses}')\n",
        "            print(f'Minimum Validation Loss: {min_val_loss}')\n",
        "            print(f'Validation Losses: {val_losses}')\n",
        "            print(f'Epochs ran: {epochs_ran}')\n",
        "            timestr = CHECKPOINT_FILE\n",
        "        else:\n",
        "            os.mkdir(os.path.join('drive', 'MyDrive', 'DriveLM', 'results', timestr))\n",
        "\n",
        "        min_train_loss, min_val_loss = custom_train(min_train_loss, min_val_loss, best_model, epochs_ran)\n",
        "\n",
        "        if MODEL_TYPE == 'BLIP':\n",
        "          best_model = BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base')\n",
        "        else:\n",
        "          best_model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b', device_map=\"auto\")\n",
        "\n",
        "        if LORA_ENABLED:\n",
        "          best_model = get_peft_model(best_model, lora_config)\n",
        "\n",
        "        best_model.load_state_dict(torch.load(os.path.join('drive', 'MyDrive', 'DriveLM', 'results', timestr, 'latest_model.pth')))\n",
        "        test_loss = val_model(test_dataloader) / len(test_dataloader)\n",
        "        statistics = [min_train_loss, min_val_loss, test_loss]\n",
        "        save_experiment(statistics)\n",
        "    else:\n",
        "        train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz33jn5VKD1X"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5kR1YXbomxG"
      },
      "outputs": [],
      "source": [
        "# processor = Blip2Processor.from_pretrained('Salesforce/blip2-opt-2.7b', padding_side='left')\n",
        "# model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b', torch_dtype=torch.float16, device_map=\"auto\")\n",
        "# model.to(device)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# text = ['Question: What is the ego state of the vehicle? Answer: The ego-vehicle is moving']\n",
        "# enc = processor(read_image(train_dset[0][1]).to(device), text, padding=True, truncation=True, return_tensors='pt')\n",
        "# enc.to(device)\n",
        "\n",
        "# outputs = model(**enc, labels=enc['input_ids'])\n",
        "# loss = outputs.loss\n",
        "# print(outputs.loss.item())\n",
        "\n",
        "# # Back-prop\n",
        "# loss.backward()\n",
        "# optimizer.step()\n",
        "# optimizer.zero_grad()\n",
        "\n",
        "# outputs = model(**enc, labels=enc['input_ids'])\n",
        "# print(outputs.loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgZFd-s94ajq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0hwF2CxGbgc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}